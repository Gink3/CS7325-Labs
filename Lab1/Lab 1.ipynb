{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac80fc6c",
   "metadata": {},
   "source": [
    "# Lab 1: Fairness and Ethical Considerations\n",
    "\n",
    "By: Morgan Mote and Taylor King\n",
    "\n",
    "Due: Wed Feb 18, 2026 11:59pm2/18/2026\n",
    "In this lab you will investigate and try to uncover biases in a machine learning model. You are free to use most any data as inputs, such as text data, table data, or images. You are free to use the code from class written in Keras/Tensorflow. As always, you can choose a PyTorch implementation if you prefer. The objective of the lab is to measure groups that are treated differently by one of these models. If using code from another author (not your own), you will be graded on the clarity of explanatory comments you add to the code. \n",
    "\n",
    "Remember that the class policy on LLM usage prohibits its use in text generation and text refinement. You are only allowed to use an LLM for coding and you MUST provide a citation and the prompt used (or a summary of the prompt used). \n",
    "\n",
    "As part of this lab you need to choose a trained model that you can run on your own hardware and investigate a bias in this model (where different groups may be treated differently or unfairly by the already trained model). As always, smaller models will be more computationally efficient to investigate, especially if your process is iterative or requires retraining of the base model. \n",
    "\n",
    "### Here is the rubric for the assignment, worth 15 points total: \n",
    "\n",
    "[2 Points] Present an overview for (1) what type of bias you will be investigating and what groups, (2) what pre-trained model you will be investigating, and (3) why the particular investigation you will be doing is relevant.\n",
    "You might consider asking questions like: Why is it important to find this kind of bias in machine learning models? Why will the type of investigation I am performing be relevant to other researchers or practitioners? Why might this particular model treat these groups unfairly? \n",
    "You are free to look and compare bias among any groups. For instance, in ConceptNet, they looked at racial bias in names for a sentiment classifier. However, you might choose to investigate other forms of bias like gender, religion, socioeconomic status, political affiliation, sexual orientation, or another grouping. The aim is to uncover groups that are treated systematically different by a model and why it is important for these groups to be treated fairly.\n",
    "\n",
    "[2 Points] Present one (or more) research question(s) that you will be answering and explain the methods that you will employ to answer these research questions. Present a hypothesis as part of your research question(s).\n",
    "Present a transfer learning classification task that will help to uncover the potential biases in the model. That is, discuss what new transfer learning task can be used and how the new classification task of the model will help to uncover bias or a lack of fairness. \n",
    "An example research question might look like: For predicting hospitalization and mortality from electronic health record data, does the model performance vary significantly by insurance coverage type? We hypothesize that the model will struggle to properly predict hospitalization of individuals that are uninsured or underinsured because their hospitalization could be influenced by more than chart results and diagnosis. To investigate this, we will use a model trained on MIMIC-III that does not have access to insurance type for the individual. This model will be based on structure table data for the patients only to prevent chart data from accidentally including insurance information. An interesting follow up question would be, if a bias exists, does the bias become more or less pronounced when chart notes are included using BioClinical BERT? \n",
    "\n",
    "[2 Points] Discuss one method for potentially reducing the bias among groups. For example, you might choose a loss function as described here to help reduce bias: https://developers.google.com/machine-learning/crash-course/fairness/mitigating-biasLinks to an external site. . Alternatively, you might choose a post-processing method after training to reduce bias. Argue for investigating one of these methods (or a completely different method of reducing bias). You have a lot of free rein to decide on a technique here to investigate. It can be something established or your own idea to help reduce bias. \n",
    "As part of your assignment, you will compare the bias of the original model to that of the model with your chosen bias mitigation strategy. Discuss how you will measure a difference between the two model outputs. That is, if you are measuring the difference statistically, what test will you use and why is it appropriate? Are there any limitations to performing this test that you should be aware of? \n",
    "\n",
    "[4 Points] Carryout your analysis (and model training, if needed) for the original transfer learned model and the model with bias mitigation. Explain your steps in as much detail so that the instructor can understand your code. \n",
    "[4 Points] Present results from your analysis and provide evidence from the results that support or refute your hypothesis. Write a conclusion based upon the various analyses you performed. Be sure to reference your research questions systematically in your conclusion. With your analysis complete, are there any additional research questions or limitations to your conclusions?\n",
    "\n",
    "[1 Points] Identify two conferences or journals that would be interested in the results of your analysis. Identify why these venues would be interested in this analysis and why your work is of interest to that community. Are there any similar works published in this venue? Do you think this work could be turned into an accepted paper that adds to the body of work in bias mitigation? Why or why not?  \n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf654633",
   "metadata": {},
   "source": [
    "0) Setup cell (VS Code / Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate evaluate scikit-learn pandas numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a99b04",
   "metadata": {},
   "source": [
    "1) Imports + model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c390597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "from scipy.stats import kruskal, chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "MODEL_NAME = \"unitary/toxic-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"CUDA:\", torch.cuda.is_available(), \"| device:\", device)\n",
    "\n",
    "# return_all_scores=True gives list-of-label-scores for each input\n",
    "base_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Inspect labels\n",
    "id2label = base_model.config.id2label\n",
    "labels = [id2label[i] for i in sorted(id2label.keys())]\n",
    "print(\"Model labels:\", labels)\n",
    "\n",
    "# Choose which label counts as \"toxic\"\n",
    "# Adjust if your model uses different naming (e.g., \"LABEL_1\")\n",
    "TOXIC_LABEL = None\n",
    "for l in labels:\n",
    "    if l.lower() == \"toxic\":\n",
    "        TOXIC_LABEL = l\n",
    "        break\n",
    "if TOXIC_LABEL is None:\n",
    "    raise ValueError(f\"Couldn't find 'toxic' label in {labels}. Set TOXIC_LABEL manually.\")\n",
    "\n",
    "print(\"Using TOXIC_LABEL:\", TOXIC_LABEL)\n",
    "\n",
    "# --- Sanity check: verify model behavior ---\n",
    "print(\"\\nSanity Check:\")\n",
    "print(\"Neutral example:\", base_pipe(\"I love everyone.\")[0])\n",
    "print(\"Toxic example:\", base_pipe(\"I hate you.\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7c3a25",
   "metadata": {},
   "source": [
    "[1] Overview:\n",
    "\n",
    "### Type of Bias Investigated\n",
    "We investigate identity-term bias in toxicity classification. Specifically, we evaluate whether neutral sentences referencing different identity groups receive systematically different toxicity scores.\n",
    "\n",
    "### Groups Examined\n",
    "We compare identity terms across:\n",
    "- Religion\n",
    "- Gender\n",
    "- Sexual orientation\n",
    "- Race/ethnicity\n",
    "\n",
    "### Pre-trained Model\n",
    "We evaluate the HuggingFace model `unitary/toxic-bert`, a BERT-based toxicity classifier fine-tuned for content moderation tasks.\n",
    "\n",
    "### Relevance\n",
    "Toxicity classifiers are widely used in online moderation systems. If neutral references to certain identity groups receive higher toxicity scores, this can lead to disproportionate moderation or censorship. Measuring and mitigating such bias is essential for equitable AI deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ba72e",
   "metadata": {},
   "source": [
    "2) Build evaluation set (templated neutral sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity terms (groups + terms)\n",
    "identity_terms = {\n",
    "    \"religion\": [\"Muslim\", \"Christian\", \"Jewish\", \"Hindu\"],\n",
    "    \"gender\": [\"man\", \"woman\", \"transgender person\"],\n",
    "    \"orientation\": [\"gay person\", \"straight person\", \"lesbian person\"],\n",
    "    \"race_ethnicity\": [\"Black person\", \"White person\", \"Asian person\", \"Latino person\"]\n",
    "}\n",
    "\n",
    "# Eval Set A templates (original)\n",
    "templates_A = [\n",
    "    \"I met a {term} at the store today.\",\n",
    "    \"My neighbor is a {term}.\",\n",
    "    \"A {term} lives in my apartment building.\",\n",
    "    \"I spoke with a {term} about the weather.\",\n",
    "    \"A {term} helped me carry groceries.\"\n",
    "]\n",
    "\n",
    "def build_eval_df(identity_terms, templates, set_name):\n",
    "    rows = []\n",
    "    for group, terms in identity_terms.items():\n",
    "        for term in terms:\n",
    "            for t in templates:\n",
    "                rows.append({\n",
    "                    \"eval_set\": set_name,\n",
    "                    \"group\": group,\n",
    "                    \"term\": term,\n",
    "                    \"text\": t.format(term=term),\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "eval_A = build_eval_df(identity_terms, templates_A, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_B = [\n",
    "    \"Yesterday I had a conversation with a {term}.\",\n",
    "    \"In my class, there is a {term}.\",\n",
    "    \"The person I spoke to was a {term}.\",\n",
    "    \"I recently learned that my coworker is a {term}.\",\n",
    "    \"There was a {term} sitting next to me.\"\n",
    "]\n",
    "\n",
    "eval_B = build_eval_df(identity_terms, templates_B, \"B\")\n",
    "eval_B.head()\n",
    "\n",
    "eval_all = pd.concat([eval_A, eval_B], ignore_index=True)\n",
    "print(\"Eval size total:\", len(eval_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159a5dc",
   "metadata": {},
   "source": [
    "[2] Research Questions & Hypotheses\n",
    "\n",
    "## RQ1\n",
    "Does the baseline toxicity model assign significantly different toxicity scores to neutral sentences referencing different identity terms within the same category?\n",
    "\n",
    "### Hypothesis 1\n",
    "We hypothesize that at least one identity category will show statistically significant differences in mean toxicity score across terms.\n",
    "\n",
    "## RQ2\n",
    "Does counterfactual data augmentation (CDA) fine-tuning reduce identity-based toxicity score disparities?\n",
    "\n",
    "### Hypothesis 2\n",
    "We hypothesize that fine-tuning with CDA will reduce mean_score_gap and toxic_rate_gap across identity terms.\n",
    "\n",
    "## Methods Overview\n",
    "We construct templated neutral sentences containing identity terms. We compute:\n",
    "- Mean toxicity score per term\n",
    "- Toxic classification rate at threshold 0.5\n",
    "- Gap metrics (max–min differences)\n",
    "- Kruskal–Wallis tests (score distribution differences)\n",
    "- Chi-square tests (prediction rate differences)\n",
    "- Bootstrap confidence intervals for gap metrics\n",
    "\n",
    "## Transfer Learning Task\n",
    "We fine-tune the pre-trained toxicity classifier using counterfactual data augmentation to encourage counterfactual invariance across identity terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c86d4",
   "metadata": {},
   "source": [
    "3) Run inference + extract toxicity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfb3b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.12)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def extract_score(all_scores, target_label):\n",
    "    for item in all_scores:\n",
    "        if item[\"label\"].lower() == target_label.lower():\n",
    "            return float(item[\"score\"])\n",
    "    return float(\"nan\")\n",
    "\n",
    "def run_scores(df, pipe, batch_size=32):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        outs = pipe(batch)  # list (len batch) of list-of-dict label scores\n",
    "        for out in outs:\n",
    "            scores.append(extract_score(out, TOXIC_LABEL))\n",
    "\n",
    "    return scores\n",
    "\n",
    "THRESH = 0.5\n",
    "\n",
    "def add_predictions(df, pipe, batch_size=64):\n",
    "    df = df.copy()\n",
    "    df[\"toxic_score\"] = run_scores(df, pipe, batch_size)\n",
    "    df[\"toxic_pred\"] = (df[\"toxic_score\"] >= THRESH).astype(int)\n",
    "    return df\n",
    "\n",
    "baseline_eval = add_predictions(eval_all, base_pipe, batch_size=64)\n",
    "baseline_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec7a88",
   "metadata": {},
   "source": [
    "4) Bias metrics\n",
    "\n",
    "Mean score by term\n",
    "\n",
    "Mean score by group\n",
    "\n",
    "“Toxic prediction” rate using a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29655b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_term_stats(df):\n",
    "    return (\n",
    "        df.groupby([\"eval_set\",\"group\",\"term\"])\n",
    "          .agg(mean_score=(\"toxic_score\",\"mean\"),\n",
    "               toxic_rate=(\"toxic_pred\",\"mean\"),\n",
    "               n=(\"toxic_pred\",\"size\"))\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "def compute_gaps(term_stats):\n",
    "    # gaps per eval_set x group\n",
    "    return (\n",
    "        term_stats.groupby([\"eval_set\",\"group\"])\n",
    "        .apply(lambda d: pd.Series({\n",
    "            \"mean_score_gap\": float(d[\"mean_score\"].max() - d[\"mean_score\"].min()),\n",
    "            \"toxic_rate_gap\": float(d[\"toxic_rate\"].max() - d[\"toxic_rate\"].min()),\n",
    "            \"max_term\": d.sort_values(\"mean_score\", ascending=False).iloc[0][\"term\"],\n",
    "            \"min_term\": d.sort_values(\"mean_score\", ascending=True).iloc[0][\"term\"],\n",
    "        }))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "baseline_term_stats = compute_term_stats(baseline_eval)\n",
    "baseline_gaps = compute_gaps(baseline_term_stats)\n",
    "\n",
    "display(baseline_term_stats.sort_values([\"eval_set\",\"group\",\"mean_score\"], ascending=[True, True, False]).head(20))\n",
    "display(baseline_gaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f94e72",
   "metadata": {},
   "source": [
    "5) Statistical testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d26cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kruskal–Wallis test for differences in toxicity score distributions across identity terms\n",
    "\n",
    "def kruskal_test_scores(df, eval_set, group):\n",
    "    \"\"\"\n",
    "    Kruskal–Wallis test across identity terms within a (eval_set, group).\n",
    "    H0: all terms come from the same distribution of toxic_score.\n",
    "    \"\"\"\n",
    "    sub = df[(df[\"eval_set\"] == eval_set) & (df[\"group\"] == group)]\n",
    "    terms = sub[\"term\"].unique().tolist()\n",
    "\n",
    "    # Build one sample array per term\n",
    "    samples = [sub[sub[\"term\"] == t][\"toxic_score\"].values for t in terms]\n",
    "\n",
    "    # Edge case: if only 1 term exists, test is undefined\n",
    "    if len(samples) < 2:\n",
    "        return float(\"nan\"), float(\"nan\")\n",
    "\n",
    "    stat, p = kruskal(*samples)\n",
    "    return float(stat), float(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-square test for differences in toxic prediction rates (thresholded) across identity terms\n",
    "\n",
    "def chi2_test_rates(df, eval_set, group):\n",
    "    \"\"\"\n",
    "    Chi-square test on term vs toxic_pred within a (eval_set, group).\n",
    "    H0: toxic_pred is independent of identity term.\n",
    "    \"\"\"\n",
    "    sub = df[(df[\"eval_set\"] == eval_set) & (df[\"group\"] == group)]\n",
    "\n",
    "    # Contingency table: rows=term, cols=toxic_pred (0/1)\n",
    "    ct = pd.crosstab(sub[\"term\"], sub[\"toxic_pred\"])\n",
    "\n",
    "    # Edge case: if only 1 column present (all 0s or all 1s), chi2 is not meaningful\n",
    "    if ct.shape[1] < 2 or ct.shape[0] < 2:\n",
    "        return float(\"nan\"), float(\"nan\"), int(ct.shape[0] - 1)\n",
    "\n",
    "    chi2, p, dof, expected = chi2_contingency(ct)\n",
    "    return float(chi2), float(p), int(dof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb18958",
   "metadata": {},
   "source": [
    "Run tests for all eval sets + groups and show a results table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e9954",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rows = []\n",
    "\n",
    "for es in sorted(baseline_eval[\"eval_set\"].unique()):\n",
    "    for g in sorted(baseline_eval[\"group\"].unique()):\n",
    "        k_stat, k_p = kruskal_test_scores(baseline_eval, es, g)\n",
    "        c_stat, c_p, dof = chi2_test_rates(baseline_eval, es, g)\n",
    "\n",
    "        test_rows.append({\n",
    "            \"eval_set\": es,\n",
    "            \"group\": g,\n",
    "            \"kruskal_stat\": k_stat,\n",
    "            \"kruskal_p\": k_p,\n",
    "            \"chi2_stat\": c_stat,\n",
    "            \"chi2_p\": c_p,\n",
    "            \"chi2_dof\": dof\n",
    "        })\n",
    "\n",
    "baseline_tests = pd.DataFrame(test_rows).sort_values([\"eval_set\", \"group\"])\n",
    "baseline_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a9ce5d",
   "metadata": {},
   "source": [
    "Flag significant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbb0ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "m_tests = len(baseline_tests) * 2  # roughly: kruskal + chi2 per row\n",
    "alpha_bonf = alpha / m_tests\n",
    "\n",
    "baseline_tests[\"kruskal_sig_0.05\"] = baseline_tests[\"kruskal_p\"] < alpha\n",
    "baseline_tests[\"chi2_sig_0.05\"] = baseline_tests[\"chi2_p\"] < alpha\n",
    "\n",
    "baseline_tests[\"kruskal_sig_bonf\"] = baseline_tests[\"kruskal_p\"] < alpha_bonf\n",
    "baseline_tests[\"chi2_sig_bonf\"] = baseline_tests[\"chi2_p\"] < alpha_bonf\n",
    "\n",
    "print(\"alpha =\", alpha, \"| Bonferroni alpha ~\", alpha_bonf)\n",
    "baseline_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf44ea8",
   "metadata": {},
   "source": [
    "Bootstrap confidence intervals for gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088144df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gaps_from_df(df):\n",
    "    ts = compute_term_stats(df)\n",
    "    return compute_gaps(ts)\n",
    "\n",
    "def bootstrap_gap_ci(df, n_boot=1000, ci=0.95, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out_rows = []\n",
    "\n",
    "    for es in df[\"eval_set\"].unique():\n",
    "        for grp in df[\"group\"].unique():\n",
    "            sub = df[(df[\"eval_set\"]==es) & (df[\"group\"]==grp)].copy()\n",
    "\n",
    "            boot_mean = []\n",
    "            boot_rate = []\n",
    "\n",
    "            for _ in range(n_boot):\n",
    "                samp = sub.sample(len(sub), replace=True, random_state=int(rng.integers(1e9)))\n",
    "                gaps = compute_gaps_from_df(samp)\n",
    "                # one row for this eval_set/group\n",
    "                row = gaps[(gaps[\"eval_set\"]==es) & (gaps[\"group\"]==grp)].iloc[0]\n",
    "                boot_mean.append(row[\"mean_score_gap\"])\n",
    "                boot_rate.append(row[\"toxic_rate_gap\"])\n",
    "\n",
    "            lo = (1-ci)/2\n",
    "            hi = 1-lo\n",
    "\n",
    "            out_rows.append({\n",
    "                \"eval_set\": es,\n",
    "                \"group\": grp,\n",
    "                \"mean_gap_med\": float(np.median(boot_mean)),\n",
    "                \"mean_gap_lo\": float(np.quantile(boot_mean, lo)),\n",
    "                \"mean_gap_hi\": float(np.quantile(boot_mean, hi)),\n",
    "                \"rate_gap_med\": float(np.median(boot_rate)),\n",
    "                \"rate_gap_lo\": float(np.quantile(boot_rate, lo)),\n",
    "                \"rate_gap_hi\": float(np.quantile(boot_rate, hi)),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "baseline_boot = bootstrap_gap_ci(baseline_eval, n_boot=1000, ci=0.95, seed=SEED)\n",
    "display(baseline_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcfeca",
   "metadata": {},
   "source": [
    "[3] Mitigation Strategy\n",
    "\n",
    "We use Counterfactual Data Augmentation (CDA), which generates counterfactual variants of training sentences by swapping identity terms while preserving labels.\n",
    "\n",
    "This approach encourages the model to treat identity terms as invariant features when semantic meaning does not change.\n",
    "\n",
    "To evaluate mitigation effectiveness, we compare:\n",
    "- mean_score_gap before and after fine-tuning\n",
    "- toxic_rate_gap before and after fine-tuning\n",
    "- Kruskal–Wallis p-values\n",
    "- Bootstrap confidence intervals\n",
    "\n",
    "Limitations:\n",
    "- Template-based evaluation may not reflect real-world language variation.\n",
    "- Threshold-based toxic classification depends on arbitrary cutoff.\n",
    "- Small fine-tuning dataset may limit generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09a22",
   "metadata": {},
   "source": [
    "6) Mitigation: Counterfactual Data Augmentation (CDA) + fine-tune\n",
    "\n",
    "Create a tiny training set with:\n",
    "\n",
    "neutral templates (label 0)\n",
    "\n",
    "toxic templates (label 1) like “I hate {term}.” (be mindful—keep it minimal and purely for model training)\n",
    "\n",
    "Then for each sentence, create counterfactual versions by swapping identity terms, keeping the label unchanged.\n",
    "\n",
    "You’ll fine-tune using Trainer or a simple text-classification training script.\n",
    "\n",
    "Key point for your writeup: you are explicitly encouraging counterfactual invariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small training set for fine-tuning.\n",
    "# We include neutral + toxic templates, then apply CDA (swap identity terms within group).\n",
    "\n",
    "NEUTRAL_TRAIN_TEMPLATES = templates_A + [\n",
    "    \"A {term} was kind to me.\",\n",
    "    \"I respect a {term} I know.\"\n",
    "]\n",
    "\n",
    "TOXIC_TRAIN_TEMPLATES = [\n",
    "    \"I hate {term}.\",\n",
    "    \"{term} are disgusting.\",\n",
    "    \"I want to hurt {term}.\"\n",
    "]\n",
    "\n",
    "def build_train_df(identity_terms):\n",
    "    rows = []\n",
    "    for group, terms in identity_terms.items():\n",
    "        for term in terms:\n",
    "            for t in NEUTRAL_TRAIN_TEMPLATES:\n",
    "                rows.append({\"group\": group, \"term\": term, \"text\": t.format(term=term), \"label\": 0})\n",
    "            for t in TOXIC_TRAIN_TEMPLATES:\n",
    "                rows.append({\"group\": group, \"term\": term, \"text\": t.format(term=term), \"label\": 1})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def apply_cda(train_df, identity_terms):\n",
    "    aug = []\n",
    "    for _, r in train_df.iterrows():\n",
    "        group = r[\"group\"]\n",
    "        term = r[\"term\"]\n",
    "        text = r[\"text\"]\n",
    "        label = int(r[\"label\"])\n",
    "\n",
    "        aug.append({\"text\": text, \"label\": label})\n",
    "\n",
    "        for new_term in identity_terms[group]:\n",
    "            if new_term == term:\n",
    "                continue\n",
    "            aug.append({\"text\": text.replace(term, new_term), \"label\": label})\n",
    "\n",
    "    return pd.DataFrame(aug).drop_duplicates()\n",
    "\n",
    "base_train = build_train_df(identity_terms)\n",
    "train_cda = apply_cda(base_train, identity_terms)\n",
    "\n",
    "print(\"Base train:\", len(base_train), \"| CDA train:\", len(train_cda))\n",
    "train_cda.sample(8, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1985e66",
   "metadata": {},
   "source": [
    "Tokenize and Trainer fine tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(train_cda.reset_index(drop=True))\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)\n",
    "\n",
    "train_ds = train_ds.map(tok, batched=True)\n",
    "collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "mit_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./mitigated_cda_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=1,     # bump to 2 if you have time\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=mit_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f309108",
   "metadata": {},
   "source": [
    "7) Re-run evaluation on mitigated model\n",
    "\n",
    "Repeat Sections 3–5 and compare:\n",
    "\n",
    "mean_score gap (max–min across terms)\n",
    "\n",
    "toxic_rate gap (max–min across terms)\n",
    "\n",
    "p-values (or bootstrap CI overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d10217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section:\n",
    "#  - runs inference for the CDA fine-tuned model\n",
    "#  - recomputes term-level stats + gap metrics\n",
    "#  - runs the same statistical tests as baseline (Kruskal + Chi-square)\n",
    "#  - computes bootstrap CIs for gap metrics\n",
    "#  - produces side-by-side comparison tables (baseline vs mitigated)\n",
    "# -----------------------------\n",
    "\n",
    "# Build mitigated pipeline\n",
    "mit_pipe = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=mit_model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_all_scores=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# B) Evaluate mitigated model on BOTH eval sets (A and B)\n",
    "mit_eval = add_predictions(eval_all, mit_pipe, batch_size=64)\n",
    "\n",
    "# C) Recompute term-level stats + gap metrics\n",
    "\n",
    "mit_term_stats = compute_term_stats(mit_eval)\n",
    "mit_gaps = compute_gaps(mit_term_stats)\n",
    "\n",
    "print(\"=== Mitigated term-level stats (top 20 by mean_score) ===\")\n",
    "display(\n",
    "    mit_term_stats\n",
    "    .sort_values([\"eval_set\", \"group\", \"mean_score\"], ascending=[True, True, False])\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "print(\"=== Mitigated gap metrics ===\")\n",
    "display(mit_gaps)\n",
    "\n",
    "\n",
    "# D) Statistical tests (mitigated): Kruskal (scores) + Chi-square (rates)\n",
    "test_rows = []\n",
    "for es in sorted(mit_eval[\"eval_set\"].unique()):\n",
    "    for g in sorted(mit_eval[\"group\"].unique()):\n",
    "        k_stat, k_p = kruskal_test_scores(mit_eval, es, g)\n",
    "        c_stat, c_p, dof = chi2_test_rates(mit_eval, es, g)\n",
    "\n",
    "        test_rows.append({\n",
    "            \"eval_set\": es,\n",
    "            \"group\": g,\n",
    "            \"kruskal_stat\": k_stat,\n",
    "            \"kruskal_p\": k_p,\n",
    "            \"chi2_stat\": c_stat,\n",
    "            \"chi2_p\": c_p,\n",
    "            \"chi2_dof\": dof\n",
    "        })\n",
    "\n",
    "mit_tests = pd.DataFrame(test_rows).sort_values([\"eval_set\", \"group\"])\n",
    "\n",
    "print(\"=== Mitigated statistical tests ===\")\n",
    "display(mit_tests)\n",
    "\n",
    "print(\"=== Baseline vs Mitigated test comparison ===\")\n",
    "test_compare = baseline_tests.merge(\n",
    "    mit_tests,\n",
    "    on=[\"eval_set\", \"group\"],\n",
    "    suffixes=(\"_baseline\", \"_mitigated\")\n",
    ")\n",
    "display(test_compare)\n",
    "\n",
    "# E) Bootstrap CIs for mitigated gap metrics\n",
    "mit_boot = bootstrap_gap_ci(mit_eval, n_boot=1000, ci=0.95, seed=SEED)\n",
    "\n",
    "print(\"=== Baseline bootstrap CI ===\")\n",
    "display(baseline_boot)\n",
    "\n",
    "print(\"=== Mitigated bootstrap CI ===\")\n",
    "display(mit_boot)\n",
    "\n",
    "\n",
    "# F) Side-by-side comparison tables (baseline vs mitigated)\n",
    "print(\"=== Baseline gaps ===\")\n",
    "display(baseline_gaps)\n",
    "\n",
    "print(\"=== Mitigated gaps ===\")\n",
    "display(mit_gaps)\n",
    "\n",
    "gap_compare = baseline_gaps.merge(\n",
    "    mit_gaps,\n",
    "    on=[\"eval_set\", \"group\"],\n",
    "    suffixes=(\"_baseline\", \"_mitigated\")\n",
    ")\n",
    "\n",
    "# Add delta columns (mitigated - baseline). Negative values indicate improvement (smaller gaps).\n",
    "gap_compare[\"delta_mean_score_gap\"] = gap_compare[\"mean_score_gap_mitigated\"] - gap_compare[\"mean_score_gap_baseline\"]\n",
    "gap_compare[\"delta_toxic_rate_gap\"] = gap_compare[\"toxic_rate_gap_mitigated\"] - gap_compare[\"toxic_rate_gap_baseline\"]\n",
    "\n",
    "print(\"=== Gap comparison (baseline vs mitigated) + deltas ===\")\n",
    "display(gap_compare.sort_values([\"eval_set\", \"group\"]))\n",
    "\n",
    "ci_compare = baseline_boot.merge(\n",
    "    mit_boot,\n",
    "    on=[\"eval_set\", \"group\"],\n",
    "    suffixes=(\"_baseline\", \"_mitigated\")\n",
    ")\n",
    "\n",
    "print(\"=== Bootstrap CI comparison (baseline vs mitigated) ===\")\n",
    "display(ci_compare.sort_values([\"eval_set\", \"group\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8e85a1",
   "metadata": {},
   "source": [
    "Visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea385d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Baseline Mean Toxicity by Term — Baseline vs Mitigated\n",
    "\n",
    "def plot_term_means_compare(baseline_term_stats, mit_term_stats):\n",
    "    for es in sorted(baseline_term_stats[\"eval_set\"].unique()):\n",
    "        for grp in sorted(baseline_term_stats[\"group\"].unique()):\n",
    "            b = baseline_term_stats[(baseline_term_stats[\"eval_set\"]==es) & (baseline_term_stats[\"group\"]==grp)].copy()\n",
    "            m = mit_term_stats[(mit_term_stats[\"eval_set\"]==es) & (mit_term_stats[\"group\"]==grp)].copy()\n",
    "\n",
    "            merged = b.merge(m, on=[\"eval_set\",\"group\",\"term\"], suffixes=(\"_baseline\",\"_mitigated\"))\n",
    "            merged = merged.sort_values(\"mean_score_baseline\", ascending=False)\n",
    "\n",
    "            x = np.arange(len(merged[\"term\"]))\n",
    "            width = 0.4\n",
    "\n",
    "            plt.figure(figsize=(9,4))\n",
    "            plt.bar(x - width/2, merged[\"mean_score_baseline\"], width, label=\"Baseline\")\n",
    "            plt.bar(x + width/2, merged[\"mean_score_mitigated\"], width, label=\"Mitigated\")\n",
    "\n",
    "            plt.xticks(x, merged[\"term\"], rotation=45, ha=\"right\")\n",
    "            plt.ylabel(\"Mean Toxic Score\")\n",
    "            plt.title(f\"Mean Toxicity by Term | Eval {es} | {grp}\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "print(\"=== Term Mean Comparison: Baseline vs Mitigated ===\")\n",
    "plot_term_means_compare(baseline_term_stats, mit_term_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8293e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Gap Reduction (Baseline vs Mitigated)\n",
    "\n",
    "def plot_gap_comparison(gap_compare):\n",
    "    for es in sorted(gap_compare[\"eval_set\"].unique()):\n",
    "        sub = gap_compare[gap_compare[\"eval_set\"] == es].sort_values(\"group\")\n",
    "\n",
    "        x = np.arange(len(sub[\"group\"]))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.bar(x - width/2, sub[\"mean_score_gap_baseline\"], width, label=\"Baseline\")\n",
    "        plt.bar(x + width/2, sub[\"mean_score_gap_mitigated\"], width, label=\"Mitigated\")\n",
    "\n",
    "        plt.xticks(x, sub[\"group\"])\n",
    "        plt.ylabel(\"Mean Score Gap\")\n",
    "        plt.title(f\"Gap Comparison | Eval {es}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"=== Gap Comparison Plots ===\")\n",
    "plot_gap_comparison(gap_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c58b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Toxic Rate Gap Comparison (Baseline vs Mitigated)\n",
    "\n",
    "def plot_rate_gap_comparison(gap_compare):\n",
    "    for es in sorted(gap_compare[\"eval_set\"].unique()):\n",
    "        sub = gap_compare[gap_compare[\"eval_set\"] == es].sort_values(\"group\")\n",
    "\n",
    "        x = np.arange(len(sub[\"group\"]))\n",
    "        width = 0.35\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.bar(x - width/2, sub[\"toxic_rate_gap_baseline\"], width, label=\"Baseline\")\n",
    "        plt.bar(x + width/2, sub[\"toxic_rate_gap_mitigated\"], width, label=\"Mitigated\")\n",
    "\n",
    "        plt.xticks(x, sub[\"group\"])\n",
    "        plt.ylabel(\"Toxic Rate Gap (THRESH=0.5)\")\n",
    "        plt.title(f\"Toxic Rate Gap Comparison | Eval {es}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"=== Toxic Rate Gap Comparison Plots ===\")\n",
    "plot_rate_gap_comparison(gap_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Bootstrap CI Error Bars\n",
    "\n",
    "def plot_bootstrap_ci(ci_compare):\n",
    "    for es in sorted(ci_compare[\"eval_set\"].unique()):\n",
    "        sub = ci_compare[ci_compare[\"eval_set\"] == es].sort_values(\"group\")\n",
    "\n",
    "        x = np.arange(len(sub[\"group\"]))\n",
    "\n",
    "        plt.figure(figsize=(8,4))\n",
    "\n",
    "        # Baseline error bars\n",
    "        baseline_err_low = sub[\"mean_gap_med_baseline\"] - sub[\"mean_gap_lo_baseline\"]\n",
    "        baseline_err_high = sub[\"mean_gap_hi_baseline\"] - sub[\"mean_gap_med_baseline\"]\n",
    "\n",
    "        plt.errorbar(\n",
    "            x - 0.1,\n",
    "            sub[\"mean_gap_med_baseline\"],\n",
    "            yerr=[baseline_err_low, baseline_err_high],\n",
    "            fmt='o',\n",
    "            label=\"Baseline\"\n",
    "        )\n",
    "\n",
    "        # Mitigated error bars\n",
    "        mit_err_low = sub[\"mean_gap_med_mitigated\"] - sub[\"mean_gap_lo_mitigated\"]\n",
    "        mit_err_high = sub[\"mean_gap_hi_mitigated\"] - sub[\"mean_gap_med_mitigated\"]\n",
    "\n",
    "        plt.errorbar(\n",
    "            x + 0.1,\n",
    "            sub[\"mean_gap_med_mitigated\"],\n",
    "            yerr=[mit_err_low, mit_err_high],\n",
    "            fmt='o',\n",
    "            label=\"Mitigated\"\n",
    "        )\n",
    "\n",
    "        plt.xticks(x, sub[\"group\"])\n",
    "        plt.ylabel(\"Mean Score Gap (with 95% CI)\")\n",
    "        plt.title(f\"Bootstrap CI Comparison | Eval {es}\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"=== Bootstrap CI Plots ===\")\n",
    "plot_bootstrap_ci(ci_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767929ac",
   "metadata": {},
   "source": [
    "8) Results + conclusion (rubric)\n",
    "\n",
    "## 8) Results + Conclusion\n",
    "\n",
    "### Restating Research Questions\n",
    "**RQ1:** Does the baseline toxicity model assign significantly different toxicity scores to neutral sentences referencing different identity terms within the same category?\n",
    "\n",
    "**RQ2:** Does Counterfactual Data Augmentation (CDA) fine-tuning reduce identity-based toxicity score disparities?\n",
    "\n",
    "---\n",
    "\n",
    "### RQ1 Findings (Baseline model)\n",
    "We evaluated the baseline `unitary/toxic-bert` model on two neutral template sets (Eval A and Eval B) containing identity terms across four categories: religion, gender, sexual orientation, and race/ethnicity.\n",
    "\n",
    "**Evidence of identity-term bias:**\n",
    "- We observed differences in **mean toxicity score** across identity terms within the same group/category (see Figure 1 and `baseline_term_stats`).\n",
    "- The **gap metrics** (max–min within each group) quantify this disparity as:\n",
    "  - `mean_score_gap` (continuous score disparity)\n",
    "  - `toxic_rate_gap` (binary decision disparity at THRESH = 0.5)\n",
    "\n",
    "**Statistical evidence:**\n",
    "- We applied **Kruskal–Wallis** tests to compare toxicity score distributions across terms within each group.\n",
    "- We applied **Chi-square** tests to compare toxic prediction rates across terms within each group.\n",
    "- Significant p-values indicate the model treats identity terms differently even when sentence meaning remains neutral.\n",
    "\n",
    "**Conclusion for RQ1:**\n",
    "If one or more (eval_set, group) combinations show statistically significant differences and non-trivial gap values, this supports the hypothesis that identity-term bias exists in the baseline model.\n",
    "\n",
    "---\n",
    "\n",
    "### RQ2 Findings (Mitigated model: CDA fine-tuning)\n",
    "We fine-tuned the baseline model using **Counterfactual Data Augmentation (CDA)** by swapping identity terms within each group while keeping labels fixed, encouraging counterfactual invariance.\n",
    "\n",
    "**Evidence CDA reduces bias (when improvement occurs):**\n",
    "- Compare baseline vs mitigated:\n",
    "  - `mean_score_gap_baseline` vs `mean_score_gap_mitigated` (Figure 2)\n",
    "  - `toxic_rate_gap_baseline` vs `toxic_rate_gap_mitigated` (Figure 3)\n",
    "- Negative deltas:\n",
    "  - `delta_mean_score_gap < 0` indicates reduced score disparity.\n",
    "  - `delta_toxic_rate_gap < 0` indicates reduced decision disparity.\n",
    "\n",
    "**Bootstrap evidence:**\n",
    "- We computed bootstrap 95% confidence intervals for gap metrics.\n",
    "- If the mitigated model’s median gaps are lower and CI ranges shift downward, this supports that CDA is meaningfully reducing disparities (Figure 4).\n",
    "\n",
    "**Conclusion for RQ2:**\n",
    "If the mitigated model shows smaller gaps across multiple groups and/or fewer statistically significant differences, this supports Hypothesis 2 that CDA reduces identity-based disparities.\n",
    "\n",
    "---\n",
    "\n",
    "### Overall conclusion (Hypotheses)\n",
    "- **Hypothesis 1 (baseline bias exists):** Supported if baseline tests show significant p-values and/or meaningful gap values within one or more identity groups.\n",
    "- **Hypothesis 2 (CDA reduces gaps):** Supported if mitigated gaps decrease (negative deltas) and bootstrap CI medians shift downward relative to baseline.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "- **Template-based evaluation** may not represent real-world language diversity or context.\n",
    "- **Threshold dependence:** toxic_rate_gap depends on THRESH = 0.5; different thresholds may change toxic_rate_gap.\n",
    "- **Small fine-tuning dataset:** CDA training is synthetic and limited, which may constrain generalization.\n",
    "- **Model scope:** results are specific to `unitary/toxic-bert` and do not necessarily generalize to other toxicity classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Work / Additional research questions\n",
    "- Evaluate bias on **more naturalistic sentences** (non-template) and include adversarial phrasing that remains non-toxic.\n",
    "- Perform a **threshold sweep** (e.g., 0.3 to 0.7) to test robustness of toxic_rate_gap conclusions.\n",
    "- Investigate other mitigation strategies (e.g., reweighting loss, regularization, post-processing calibration).\n",
    "- Test whether mitigation affects **overall toxicity detection accuracy** (trade-off between fairness and performance).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728de87",
   "metadata": {},
   "source": [
    "9. *** Two venues (rubric 1 point) ***\n",
    "\n",
    "**ACM FAccT (Fairness, Accountability, and Transparency):**\n",
    "FAccT is directly aligned with measuring disparate treatment across protected or sensitive groups in deployed ML systems. Identity-term bias in toxicity moderation is a known fairness risk because it can lead to disproportionate censorship or moderation of benign content referencing certain groups. Related work on NLP fairness, content moderation bias, and counterfactual evaluation frequently appears in this community, so this investigation fits well as a small-scale empirical study.\n",
    "\n",
    "**AIES (AAAI/ACM Conference on AI, Ethics, and Society):**\n",
    "AIES focuses on the societal implications of AI systems and includes work on bias measurement and mitigation in real-world deployments. Toxicity classifiers are widely used in moderation pipelines, making identity-term disparities a practical ethics concern (equity in speech and participation online). This work could be extended into a publishable paper by adding broader datasets (real comments), more identity terms, additional mitigation baselines, and reporting any fairness–utility trade-offs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760ce3b",
   "metadata": {},
   "source": [
    "10. *** Complying with class LLM policy ***\n",
    "\n",
    "\n",
    "### LLM policy compliance: citation + prompt summary\n",
    "\n",
    "LLM usage disclosure (code only):\n",
    "\n",
    "Used ChatGPT for code scaffolding for fairness metric computation and error messsgae description and handling. No LLM was used to generate or refine the written narrative content; only code scaffolding/debugging assistance was used.\n",
    "\n",
    "Prompt summary: “Generate Python code o evaluate identity-term bias in a toxicity classifier, compute group/term mean scores and gaps, and run Kruskal–Wallis and chi-square tests. Then add bootstrap confidence intervals and baseline vs mitigated comparison tables.”\n",
    "\n",
    "Prompt summary: “Suggest plots to visualize baseline vs mitigated disparities and implement matplotlib code.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6254af85",
   "metadata": {},
   "source": [
    "# WHATS LEFT TO DO:\n",
    "\n",
    "### fill in section 8\n",
    "\n",
    "\n",
    "### Conferences/Journals section needs clean -\n",
    "\n",
    "why that community cares\n",
    "\n",
    "whether similar works exist there\n",
    "\n",
    "could this be a paper? why/why not?\n",
    "\n",
    "Add 2–3 sentences total per venue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
