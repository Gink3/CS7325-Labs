{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac80fc6c",
   "metadata": {},
   "source": [
    "Lab 1: Fairness and Ethical Considerations\n",
    "\n",
    "By: Morgan Mote and Taylor King\n",
    "\n",
    "Due: Wed Feb 18, 2026 11:59pm2/18/2026\n",
    "In this lab you will investigate and try to uncover biases in a machine learning model. You are free to use most any data as inputs, such as text data, table data, or images. You are free to use the code from class written in Keras/Tensorflow. As always, you can choose a PyTorch implementation if you prefer. The objective of the lab is to measure groups that are treated differently by one of these models. If using code from another author (not your own), you will be graded on the clarity of explanatory comments you add to the code. \n",
    "\n",
    "Remember that the class policy on LLM usage prohibits its use in text generation and text refinement. You are only allowed to use an LLM for coding and you MUST provide a citation and the prompt used (or a summary of the prompt used). \n",
    "\n",
    "As part of this lab you need to choose a trained model that you can run on your own hardware and investigate a bias in this model (where different groups may be treated differently or unfairly by the already trained model). As always, smaller models will be more computationally efficient to investigate, especially if your process is iterative or requires retraining of the base model. \n",
    "\n",
    "Here is the rubric for the assignment, worth 15 points total: \n",
    "\n",
    "[2 Points] Present an overview for (1) what type of bias you will be investigating and what groups, (2) what pre-trained model you will be investigating, and (3) why the particular investigation you will be doing is relevant.\n",
    "You might consider asking questions like: Why is it important to find this kind of bias in machine learning models? Why will the type of investigation I am performing be relevant to other researchers or practitioners? Why might this particular model treat these groups unfairly? \n",
    "You are free to look and compare bias among any groups. For instance, in ConceptNet, they looked at racial bias in names for a sentiment classifier. However, you might choose to investigate other forms of bias like gender, religion, socioeconomic status, political affiliation, sexual orientation, or another grouping. The aim is to uncover groups that are treated systematically different by a model and why it is important for these groups to be treated fairly.\n",
    "\n",
    "[2 Points] Present one (or more) research question(s) that you will be answering and explain the methods that you will employ to answer these research questions. Present a hypothesis as part of your research question(s).\n",
    "Present a transfer learning classification task that will help to uncover the potential biases in the model. That is, discuss what new transfer learning task can be used and how the new classification task of the model will help to uncover bias or a lack of fairness. \n",
    "An example research question might look like: For predicting hospitalization and mortality from electronic health record data, does the model performance vary significantly by insurance coverage type? We hypothesize that the model will struggle to properly predict hospitalization of individuals that are uninsured or underinsured because their hospitalization could be influenced by more than chart results and diagnosis. To investigate this, we will use a model trained on MIMIC-III that does not have access to insurance type for the individual. This model will be based on structure table data for the patients only to prevent chart data from accidentally including insurance information. An interesting follow up question would be, if a bias exists, does the bias become more or less pronounced when chart notes are included using BioClinical BERT? \n",
    "\n",
    "[2 Points] Discuss one method for potentially reducing the bias among groups. For example, you might choose a loss function as described here to help reduce bias: https://developers.google.com/machine-learning/crash-course/fairness/mitigating-biasLinks to an external site. . Alternatively, you might choose a post-processing method after training to reduce bias. Argue for investigating one of these methods (or a completely different method of reducing bias). You have a lot of free rein to decide on a technique here to investigate. It can be something established or your own idea to help reduce bias. \n",
    "As part of your assignment, you will compare the bias of the original model to that of the model with your chosen bias mitigation strategy. Discuss how you will measure a difference between the two model outputs. That is, if you are measuring the difference statistically, what test will you use and why is it appropriate? Are there any limitations to performing this test that you should be aware of? \n",
    "\n",
    "[4 Points] Carryout your analysis (and model training, if needed) for the original transfer learned model and the model with bias mitigation. Explain your steps in as much detail so that the instructor can understand your code. \n",
    "[4 Points] Present results from your analysis and provide evidence from the results that support or refute your hypothesis. Write a conclusion based upon the various analyses you performed. Be sure to reference your research questions systematically in your conclusion. With your analysis complete, are there any additional research questions or limitations to your conclusions?\n",
    "\n",
    "[1 Points] Identify two conferences or journals that would be interested in the results of your analysis. Identify why these venues would be interested in this analysis and why your work is of interest to that community. Are there any similar works published in this venue? Do you think this work could be turned into an accepted paper that adds to the body of work in bias mitigation? Why or why not?  \n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf654633",
   "metadata": {},
   "source": [
    "0) Setup cell (VS Code / Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c14f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install transformers datasets accelerate evaluate scikit-learn pandas numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a99b04",
   "metadata": {},
   "source": [
    "1) Imports + model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c390597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "tox = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", top_k=None)  # returns all labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ba72e",
   "metadata": {},
   "source": [
    "2) Build evaluation set (templated neutral sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d7cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_terms = {\n",
    "    \"religion\": [\"Muslim\", \"Christian\", \"Jewish\", \"Hindu\"],\n",
    "    \"gender\": [\"man\", \"woman\", \"transgender person\"],\n",
    "    \"orientation\": [\"gay person\", \"straight person\", \"lesbian person\"],\n",
    "    \"race_ethnicity\": [\"Black person\", \"White person\", \"Asian person\", \"Latino person\"]\n",
    "}\n",
    "\n",
    "templates = [\n",
    "    \"I met a {term} at the store today.\",\n",
    "    \"My neighbor is a {term}.\",\n",
    "    \"A {term} lives in my apartment building.\",\n",
    "    \"I spoke with a {term} about the weather.\",\n",
    "    \"A {term} helped me carry groceries.\"\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for group, terms in identity_terms.items():\n",
    "    for term in terms:\n",
    "        for t in templates:\n",
    "            rows.append({\"group\": group, \"term\": term, \"text\": t.format(term=term)})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c86d4",
   "metadata": {},
   "source": [
    "3) Run inference + extract toxicity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfb3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxic_score(output):\n",
    "    # output is list of dicts with labels/scores\n",
    "    # find label containing \"toxic\"\n",
    "    for item in output:\n",
    "        if item[\"label\"].lower() == \"toxic\":\n",
    "            return item[\"score\"]\n",
    "    return np.nan\n",
    "\n",
    "scores = []\n",
    "for text in df[\"text\"].tolist():\n",
    "    out = tox(text)[0]  # pipeline returns list; first element is list of label dicts\n",
    "    scores.append(toxic_score(out))\n",
    "\n",
    "df[\"toxic_score\"] = scores\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec7a88",
   "metadata": {},
   "source": [
    "4) Bias metrics\n",
    "\n",
    "Mean score by term\n",
    "\n",
    "Mean score by group\n",
    "\n",
    "“Toxic prediction” rate using a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29655b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESH = 0.5\n",
    "df[\"toxic_pred\"] = (df[\"toxic_score\"] >= THRESH).astype(int)\n",
    "\n",
    "term_stats = df.groupby([\"group\",\"term\"]).agg(\n",
    "    mean_score=(\"toxic_score\",\"mean\"),\n",
    "    toxic_rate=(\"toxic_pred\",\"mean\"),\n",
    "    n=(\"toxic_pred\",\"size\")\n",
    ").reset_index()\n",
    "\n",
    "group_stats = df.groupby([\"group\"]).agg(\n",
    "    mean_score=(\"toxic_score\",\"mean\"),\n",
    "    toxic_rate=(\"toxic_pred\",\"mean\"),\n",
    "    n=(\"toxic_pred\",\"size\")\n",
    ").reset_index()\n",
    "\n",
    "term_stats.sort_values(\"mean_score\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f94e72",
   "metadata": {},
   "source": [
    "5) Statistical test (example)\n",
    "One clean test per metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "# Compare distributions across terms within a category (example: religion)\n",
    "relig = df[df[\"group\"]==\"religion\"]\n",
    "samples = [relig[relig[\"term\"]==term][\"toxic_score\"].values for term in relig[\"term\"].unique()]\n",
    "stat, p = kruskal(*samples)\n",
    "stat, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf44ea8",
   "metadata": {},
   "source": [
    "Chi-square on counts for toxic_rate differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088144df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "ct = pd.crosstab(relig[\"term\"], relig[\"toxic_pred\"])\n",
    "chi2, p, dof, expected = chi2_contingency(ct)\n",
    "chi2, p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b09a22",
   "metadata": {},
   "source": [
    "6) Mitigation: Counterfactual Data Augmentation (CDA) + fine-tune\n",
    "\n",
    "Create a tiny training set with:\n",
    "\n",
    "neutral templates (label 0)\n",
    "\n",
    "toxic templates (label 1) like “I hate {term}.” (be mindful—keep it minimal and purely for model training)\n",
    "\n",
    "Then for each sentence, create counterfactual versions by swapping identity terms, keeping the label unchanged.\n",
    "\n",
    "You’ll fine-tune using Trainer or a simple text-classification training script.\n",
    "\n",
    "Key point for your writeup: you are explicitly encouraging counterfactual invariance.\n",
    "\n",
    "(If you want, tell me whether you prefer TensorFlow/Keras or PyTorch/HF Trainer and I’ll drop in a complete, runnable fine-tuning block tailored to your environment—but the outline above is already enough to start.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f309108",
   "metadata": {},
   "source": [
    "7) Re-run evaluation on mitigated model\n",
    "\n",
    "Repeat Sections 3–5 and compare:\n",
    "\n",
    "mean_score gap (max–min across terms)\n",
    "\n",
    "toxic_rate gap (max–min across terms)\n",
    "\n",
    "p-values (or bootstrap CI overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d10217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767929ac",
   "metadata": {},
   "source": [
    "8) Results + conclusion (rubric)\n",
    "\n",
    "In markdown:\n",
    "\n",
    "Restate RQ1/RQ2\n",
    "\n",
    "Summarize key findings numerically (top 3 highest mean_score terms, pre vs post)\n",
    "\n",
    "State whether hypothesis supported\n",
    "\n",
    "Limitations: synthetic templates, threshold choice, model label interpretation, small fine-tune set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a55ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0728de87",
   "metadata": {},
   "source": [
    "Two venues (rubric 1 point)\n",
    "\n",
    "Pick two and justify:\n",
    "\n",
    "ACM FAccT (Fairness, Accountability, and Transparency)\n",
    "Interested because your work measures and mitigates differential treatment across protected groups in a deployed-style NLP setting (content moderation).\n",
    "\n",
    "AAAI/ACM Conference on AI, Ethics, and Society (AIES)\n",
    "Interested because it focuses on bias measurement + mitigation and societal impacts of AI systems like toxicity moderation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ACM FAccT\n",
    "\n",
    "from AAAI/ACM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e760ce3b",
   "metadata": {},
   "source": [
    "Complying with class LLM policy\n",
    "\n",
    "Markdown cell message on LLM usage disclosure: “Used ChatGPT for code scaffolding for HuggingFace inference + fairness metric computation.”\n",
    "\n",
    "Prompt summary example: “Generate Python code to evaluate toxicity model outputs across identity-term templates and compute group-wise mean scores and chi-square tests.”\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
